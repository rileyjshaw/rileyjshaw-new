{"componentChunkName":"component---src-templates-blog-list-js","path":"/blog/3","result":{"data":{"allMarkdownRemark":{"edges":[]},"allCombinedProjectsJson":{"nodes":[{"uid":"COMMIT_2488BCD5E5610F692773E7815FABDB247ECE55F5","title":"Bootstrap an independent data scraper","date":"2019-08-31","link":"http://rileyjshaw.commit--blog.com/rileyjshaw/rileyjshaw-new/2488bcd5e5610f692773e7815fabdb247ece55f5","repo":"rileyjshaw/rileyjshaw-new","description":"<h1>Project scraper</h1>\n<p>The projects on my site are automatically scraped and formatted at publish time using the scripts in this directory. Read more about my reasoning below, or skip to the <a href=\"#structure\">directory structure</a>.</p>\n<h2>Why?</h2>\n<p>Gatsby's source and transformer plugins are powerful, and I used them in the initial development of this site. I eventually decided that separating my collection process would be good for flexibility, control, and offline work.</p>\n<h3>Flexibility</h3>\n<p>GraphQL's filters and transforms are powerful, and Gatsby's APIs add more options for how data is fetched, cached, and transformed. However, complicated or non-standard data transforms and sanitization are much easier outside of Gatsby's ecosystem. For instance, the API starts to feel clunky for one-off treatment of specific content nodes,</p>\n<h3>Control</h3>\n<p>I've had a good experience with Gatsby but I may decide to migrate my site to another platform or format someday. Keeping my data entirely separate from the from the site's framework makes migrating my data as easy as copy/pasting this directory. It's just a few JS files!</p>\n<h3>Offline</h3>\n<p>Gatsby stores requests made through its source plugins in the <code>.cache</code> directory by default. The <code>.cache</code> directory is deleted after:</p>\n<ul>\n<li><code>gatsby clean</code> is called.</li>\n<li><code>package.json</code> changes, for example a dependency is updated or added.</li>\n<li><code>gatsby-config.js</code> changes, for example a plugin is added or modified.</li>\n<li><code>gatsby-node.js</code> changes, for example if a new Node API is invoked.</li>\n<li>â€¦etc.</li>\n</ul>\n<p>I found I was frequently triggering <code>.cache</code> wipes during development. At best this meant I was pinging APIs and atom feeds more than necessary. At worst, it made working offline with project data impossible.</p>\n<h2>Directory structure</h2>\n<p>Here's how the scraper is organized for now:</p>\n<pre><code>scrape-projects.js\n\tThe megafile to replace Gatsby's source plugins. This pulls project data\n\tfrom all online sources and saves them into `_generated/`.\n\n_generated/\n\tFiles generated by the `scrape-projects.js` above. DO NOT EDIT THESE FILES\n\tMANUALLY! They will be overwritten.\n\n\tscraped-projects-raw.json\n\t\tNot quite the raw response, but pretty close. This file\n\t\tcontains all the data that I may decide to use someday, but\n\t\thaven't yet. Organized by `type` in a nested object.\n\n    scraped-projects-formatted.json\n\t\tStandardized into a smaller format that can be smashed together with\n\t\t`curation/` data. Flattened into an array with `type` annotations on\n\t\teach node, as well as unique, unchanging project IDs (`UID`).\n\ncuration/\n\tThis is where all custom curation and processing go, eg. tagging content.\n\tProjects are modified based on their generated UID.\n\n\ttweaks.js\n\t\tMainly for one-off changes eg. fixing formatting errors from immutable\n\t\tonline sources. This file can also be used to apply changes on groups\n\t\tof files.\n\n\ttags.js\n\t\tTODO: figure out where `tags`, `lastTagged`, and `coolness` data are\n\t\tgoing to live.\n\nsources/\n\tOffline data files and collections to compliment the online data cached in\n\t`_generated/`.\n\n\tstandalone-projects.json\n\t\tTODO: Move these over from the `src/data` directory.\n\ntools/\n\tCustom tools to help classify, organize, or edit project nodes without\n\topening a text editor. Custom tools are only built for data that is too\n\tdifficult to keep updated or standardized manually.\n\tTODO: Hook these up to a Node server so they edit the JSON files directly.\n\n\ttagger.html\n\t\tFinds untagged or incorrectly tagged projects, as well as projects\n\t\tthat were last tagged before a new tag type was added. Provides an\n\t\tinterface to preview and re-tag each project.\n\n\tcool-sort.html\n\t\tTODO:Â sort or insert nodes based on their \"coolness\".\n\ntest/\n\tQuick test files to ensure data is downloaded without any dropped nodes,\n\tUIDs are unique, etc.\n</code></pre>","more":true},{"uid":"COMMIT_638C25CE03E53B1042A6E8081BBC356D865AC67C","title":"Archive pre-2019 Heroku site; Update README.md","date":"2019-08-14","link":"http://rileyjshaw.commit--blog.com/rileyjshaw/xoxo-bingo/638c25ce03e53b1042a6e8081bbc356d865ac67c","repo":"rileyjshaw/xoxo-bingo","description":"<p>Excerpt from the  new README:</p>\n<pre><code>## timeline\n2015: first bingo! [eli](https://twitter.com/veryeli) and i used the attendee\ndirectory to generate a unique card for everyone (twitter login kept it private\nðŸ”’). squares on your card were other attendees - if you met someone on your\ncard you got to check it off. we made it cuz weâ€™re shy. most of it is in the\n`pre-2019` folder!\n\n2016: we made the cards prettier by pulling in peopleâ€™s twitter photos and\ndoing imgmagick to them ðŸ”®\n\n2017: no xoxo, no bingoâ€¦ missed u all\n\n2018: xoxo was in the midst of changing their infrastructure, so i lost access\nto the attendee directory. [hannah](https://twitter.com/herlifeinpixels),\n[jason](https://twitter.com/justsomeguy) and i met in a cafe before the kickoff\nceremony and designed a static version with input from the community. hannah\nand jason made 25 icons in like two minutes, it was incredible!!!\n\n2019: i've been too cheap to get https://xoxo.bingo in previous years, but\n[andy](https://twitter.com/andymcmillan) noticed a thread on slack and hooked\nus up! thx andy.\n</code></pre>\n<p>leading up to xoxo2018, i realized we wouldn't have access to the new attendee registry. <a href=\"https://twitter.com/waxpancake\" rel=\"noopener noreferrer\" target=\"_blank\">andy</a> and i discussed ad-hoc private access and other ways to make it work, but it was too much. so hannah, jason and i made a static version with \"achievements\" sourced from the slack community.</p>\n<p>it was fun to get excited about things specific to that year, like the podcast airstream and the blue ox. and it's gonna be that way from now on! feel free to create an issue or msg on slack if you have ideas for this year's bingo squares.</p>\n<p>since it's staying a static site, i moved everything off of heroku. all site content will live in <a href=\"./2019-and-on/\" rel=\"noopener noreferrer\" target=\"_blank\"><code>2019-and-on/</code></a>.</p>","more":true},{"uid":"COMMIT_F117F4FAA75DCA5FA109BAC066ADB3B3EE5C6CF7","title":"Firehose: proof of concept","date":"2019-08-09","link":"http://rileyjshaw.commit--blog.com/rileyjshaw/rileyjshaw-new/f117f4faa75dca5fa109bac066adb3b3ee5c6cf7","repo":"rileyjshaw/rileyjshaw-new","description":"<p>I'm experimenting with auto-generating nodes for <a href=\"/lab\">https://rileyjshaw.com/lab</a> from a variety of data sources. This project may eventually replace <a href=\"/\" rel=\">https://rileyjshaw.com</a>.</p>\n<p>This is the initial commit, completed quickly as a proof of concept. There's nothing much to show, but I want to deploy ASAP so I can test the full pipeline.</p>\n<p>So far, everything has worked! Data from a variety of sources is already appearing on my local server. To reproduce:</p>\n<ul>\n<li>Run <code>gatsby develop</code>.</li>\n<li>Open <a href=\"http://localhost:8000/page-2\" rel=\"noopener noreferrer\" target=\"_blank\">http://localhost:8000/page-2</a> in a browser.</li>\n</ul>\n<p>So far, I'm surfacing data from:</p>\n<ul>\n<li>A hand-curated JSON list of projects I use for <a href=\"/lab\">https://rileyjshaw.com/lab</a>.</li>\n<li><a href=\"https://dwitter.net\" rel=\"noopener noreferrer\" target=\"_blank\">Dwitter</a>'s API.</li>\n<li><a href=\"https://rileyjshaw.commit--blog.com\" rel=\"noopener noreferrer\" target=\"_blank\">My commit--blog</a>'s Atom feed.</li>\n</ul>\n<p>Setting this up was <em>EASY</em>, which makes me excited for the future of this experiment :)</p>","more":false},{"uid":"COMMIT_1BDED12B1BBE518E87FFBAB8C0C3A4D80F10FB46","title":"Add an index for each individual project","date":"2019-08-07","link":"http://rileyjshaw.commit--blog.com/rileyjshaw/canvas/1bded12b1bbe518e87ffbab8c0c3a4d80f10fb46","repo":"rileyjshaw/canvas","description":"<p>I started this repository in the spirit of OpenFrameworks and TouchDesigner: I wanted all the libraries I might need close at hand, with a simple, abstracted API for drawing to , SVG, etc. I wanted a personal playpen / pigpen to test ideas in.</p>\n<p>For that reason, I didn't need nice features like <em>routing</em> or <em>pages.</em> :) if I wanted to see an old sketch, I'd change the root component and re-render. It worked for me!</p>\n<p>But I planned to eventually make an easier way to browse existing experiments. It would benefit me a bit, and casual viewers a lot.</p>\n<p>I haven't updated this repository in nearly two years, and I honestly never expect to again. I'm doing less browser-based creative coding these days, and trying to stretch my work in other directions.</p>\n<p>About an hour ago, I decided to create an index page or dropdown to close this project out and keep it accessible in perpetuity. When I cloned the repo and started looking at the build pipeline, I almost noped the whole idea. I built this with <code>create-react-app</code>, so even adding new pages for each project <em>the recommended way</em> involves:</p>\n<ol>\n<li>Installing some sort of React-compliant router.</li>\n<li>Spendingâ€¦ hours? figuring out which version of which router works with the project's outdated dependencies, OR,</li>\n<li>Upgrading the entire project, likely involving major upgrades to Webpack, Babel, etc.</li>\n<li>Installing something called <a href=\"https://www.npmjs.com/package/react-snapshot\" rel=\"noopener noreferrer\" target=\"_blank\">react-snapshot</a>, which apparently builds static files for you? But there's still a <code>pushState</code> history API? The README listed some tutorials, so I opened them.</li>\n<li>â€¦once I'd reached this point, I realized I'd need another method if I wanted to be done within the hour.</li>\n</ol>\n<p>At that point, I could have searched the web for \"create-react-app static routes 2017 easy\" and gone down that rabbit hole before giving up. OR, I could have given up immediately. Or I could do what I did, which was a good idea:</p>\n<p>I changed the root component 38 times by hand, typed \"npm run build\" into my terminal by hand, and dragged the built files BY HAND into unique directories that I created BY HAND.</p>\n<p>Gasp!</p>\n<p>I spent another minute in my editor surrounding the output of <code>ls -d</code> with anchor tags for a root index. (yes, by hand)</p>\n<p>The most time-intensive part of the process was writing this commit message. I'm confident if I'd tried to automate the process or rebuilt the project \"the right way\", I'd be at this for a few more hours.</p>\n<p>The result is a little sketchy. Namely, I'm sure the total payload of each page is a bit bigger, and caching takes a hit. But I think an extra kilobyte will be tolerated by the 3 people who ever visit this corner of my website.</p>\n<p>And wow it was so easy. And if I ever decide to add a new sketch, I can do the same simple steps by hand. No dependency mismatch with my local versions. No reading old docs. Just build, drag, repeat, forever.</p>\n<p>I guess I'm writing this as a reminder to myself: it's usually possible to break back out to 1995 in a pinch.</p>","more":true},{"uid":"COMMIT_5BD325E113AC998737005467529F93AED8791F45","title":"Add Dwitter data and some initial scraper options","date":"2019-08-06","link":"http://rileyjshaw.commit--blog.com/rileyjshaw/rileyjshaw.github.io/5bd325e113ac998737005467529f93aed8791f45","repo":"rileyjshaw/rileyjshaw.github.io","description":"<p>I love the tidal wave of projects on <a href=\"/lab\">/lab</a>, and I want to emphasize that for v3.0 of the website. I update pages across the web daily; Glitch, Codepen, gist.github.com, Dwitter, Hackster, etc. Plus there's social mediaâ€¦</p>\n<p>I'm okay with manual curation for the most part, but for websites like Dwitter where contributions are inherently unpolished / untitled, it doesn't make sense for me to hand-pick and manually update a giant list.</p>\n<p>Also: I'm not sure how long Dwitter will be around for. Periodically saving the underlying code / images / etc. gives me more ownership over the presentation and preservation of my data. It changes my relationship with these sites from content hosts to publishing platforms. That makes me feel more secure with my zillion links.</p>\n<p>TODO(?): Automatically fetch new content during the publishing step?</p>","more":true}]}},"pageContext":{"internalLimit":0,"internalSkip":2,"externalLimit":5,"externalSkip":8,"numPages":14,"currentPage":3}}}